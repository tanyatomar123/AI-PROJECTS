# -*- coding: utf-8 -*-
"""cemp.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GgLhvvPnGN6h849jnTtFAh1kqtEygskp
"""

pip install gradio

pip install bcrypt

import gradio as gr
import sqlite3
import bcrypt

# Database setup
def init_db():
    conn = sqlite3.connect("users.db")
    cursor = conn.cursor()
    cursor.execute("""
        CREATE TABLE IF NOT EXISTS users (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            username TEXT UNIQUE,
            password TEXT
        )
    """) # The closing triple quotes were misaligned. Fixed the indentation here.
    conn.commit()
    conn.close()

init_db()

def register(username, password):
    conn = sqlite3.connect("users.db")
    cursor = conn.cursor()
    hashed_pw = bcrypt.hashpw(password.encode(), bcrypt.gensalt())
    try:
        cursor.execute("INSERT INTO users (username, password) VALUES (?, ?)", (username, hashed_pw))
        conn.commit()
        return "‚úÖ Registration Successful! You can now log in."
    except sqlite3.IntegrityError:
        return "‚ö†Ô∏è Username already exists. Try another."
    finally:
        conn.close()

def login(username, password):
    conn = sqlite3.connect("users.db")
    cursor = conn.cursor()
    cursor.execute("SELECT password FROM users WHERE username = ?", (username,))
    result = cursor.fetchone()
    conn.close()
    if result and bcrypt.checkpw(password.encode(), result[0]):
        return "‚úÖ Login Successful! Welcome to the marketplace."
    else:
        return "‚ùå Incorrect username or password. Try again."

with gr.Blocks() as auth_app:
    gr.Markdown("# üîê Circular Economy Marketplace Login")
    with gr.Tab("Register"):
        reg_username = gr.Textbox(label="Username")
        reg_password = gr.Textbox(label="Password", type="password")
        reg_btn = gr.Button("Register")
        reg_output = gr.Textbox()
        reg_btn.click(register, inputs=[reg_username, reg_password], outputs=reg_output)

    with gr.Tab("Login"):
        log_username = gr.Textbox(label="Username")
        log_password = gr.Textbox(label="Password", type="password")
        log_btn = gr.Button("Login")
        log_output = gr.Textbox()
        log_btn.click(login, inputs=[log_username, log_password], outputs=log_output)

auth_app.launch()

import pandas as pd
import numpy as np

# Load dataset
df = pd.read_csv("/content/ecommerce_product_dataset.csv")

# Inspect dataset
print(df.head())

# Handle missing values for numeric columns only
numeric_cols = df.select_dtypes(include=np.number).columns
df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].median())



# Convert categorical variables
df = pd.get_dummies(df, columns=['Category','ProductName','DateAdded'])

from sklearn.model_selection import train_test_split

X = df.drop(columns=['StockQuantity'])  # Features
y = df['Discount']  # Target variable

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error

# Initialize model
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)

# Train model
rf_model.fit(X_train, y_train)

# Make predictions
y_pred = rf_model.predict(X_test)

# Evaluate performance
mae = mean_absolute_error(y_test, y_pred)
# Calculate MSE and then take the square root to get RMSE
mse = mean_squared_error(y_test, y_pred)  # Remove squared=False
rmse = np.sqrt(mse)  # Calculate RMSE manually

print(f"Mean Absolute Error: {mae}")
print(f"Root Mean Squared Error: {rmse}")

import tensorflow as tf
from tensorflow import keras

# Build a simple Neural Network
model = keras.Sequential([
    keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),
    keras.layers.Dense(32, activation='relu'),
    keras.layers.Dense(1)  # Output layer
])

# Compile the model
model.compile(optimizer='adam', loss='mse', metrics=['mae'])

# Train the model
model.fit(X_train, y_train, epochs=50, batch_size=16, validation_data=(X_test, y_test))

# Make predictions
y_pred_nn = model.predict(X_test)

# Evaluate performance
mse_nn = mean_squared_error(y_test, y_pred_nn)
print(f"Neural Network RMSE: {np.sqrt(mse_nn)}")

import joblib

# Save model
joblib.dump(rf_model, "product_lifecycle_model.pkl")

# Load model (when needed)
model = joblib.load("product_lifecycle_model.pkl")

pip install gradio

from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
import joblib

categorical_features = ['Category']
numeric_features = ['Price', 'Rating', 'NumReviews', 'StockQuantity', 'Discount']

preprocessor = ColumnTransformer([
    ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features),
    ('num', 'passthrough', numeric_features)
])

joblib.dump(preprocessor, "preprocessor.pkl")

import pandas as pd
import joblib
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestRegressor

# Load dataset (ensure this matches your actual dataset)
df = pd.read_csv("/content/ecommerce_product_dataset.csv")

# Define features and target variable
X = df[["Category", "ProductName", "Price", "Rating", "NumReviews", "StockQuantity", "Discount"]]
y = df["Sales"]  # Target variable (adjust based on your dataset)

# Define preprocessing (One-Hot Encoding for Category, passthrough for numeric)
categorical_features = ['Category']
numeric_features = ['Price', 'Rating', 'NumReviews', 'StockQuantity', 'Discount']

preprocessor = ColumnTransformer([
    ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features),
    ('num', 'passthrough', numeric_features)
])

# Fit the preprocessor on training data
X_transformed = preprocessor.fit_transform(X)

# Train the model
model = RandomForestRegressor(n_estimators=100, random_state=42)
model.fit(X_transformed, y)

# Save the fitted preprocessor & model
joblib.dump(preprocessor, "preprocessor.pkl")
joblib.dump(model, "product_lifecycle_model.pkl")
print("‚úÖ Model and preprocessor saved successfully!")

import gradio as gr
import joblib
import pandas as pd

# Load trained model and fitted preprocessor
model = joblib.load("product_lifecycle_model.pkl")
preprocessor = joblib.load("preprocessor.pkl")  # ‚úÖ Now pre-fitted

def preprocess_input(Category, ProductName, Price, Rating, NumReviews, StockQuantity, Discount):
    # Create DataFrame from user input
    input_df = pd.DataFrame([[Category, ProductName, Price, Rating, NumReviews, StockQuantity, Discount]],
                            columns=["Category", "ProductName", "Price", "Rating", "NumReviews", "StockQuantity", "Discount"])

    # Apply the saved preprocessor (which is already fitted)
    input_processed = preprocessor.transform(input_df)

    return input_processed

def predict_lifecycle(Category, ProductName, Price, Rating, NumReviews, StockQuantity, Discount):
    try:
        input_data = preprocess_input(Category, ProductName, Price, Rating, NumReviews, StockQuantity, Discount)
        prediction = model.predict(input_data)[0]
        return f"Predicted Product Lifecycle: {round(prediction, 2)} years"
    except Exception as e:
        return f"Error: {str(e)}"

# Create Gradio interface
iface = gr.Interface(
    fn=predict_lifecycle,
    inputs=[
        gr.Dropdown(["Plastic", "Metal", "Wood", "Composite", "Electronics"], label="Category"),
        gr.Textbox(label="Product Name"),
        gr.Number(label="Price"),
        gr.Number(label="Rating"),
        gr.Number(label="NumReviews"),
        gr.Number(label="StockQuantity"),
        gr.Number(label="Discount")
    ],
    outputs=gr.Textbox(label="Prediction"),
    title="üîÑ Product Lifecycle Prediction App",
    description="Enter product details below to predict its lifecycle duration (in years)."
)

# Launch Gradio app
iface.launch()

import joblib
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler


# Load dataset
df = pd.read_csv("/content/dynamic_pricing_data.csv")

# Encode categorical variables
label_encoders = {}
for col in ["Product Name", "Category", "Demand", "Season"]:
    le = LabelEncoder()
    df[col] = le.fit_transform(df[col])
    label_encoders[col] = le

# Save label encoders
joblib.dump(label_encoders, "label_encoders.pkl")

# Scale numerical features
scaler = StandardScaler()
num_cols = ["Base Price", "Competitor Price", "Stock", "Reviews", "Rating", "Discount"]
df[num_cols] = scaler.fit_transform(df[num_cols])

# Save scaler
joblib.dump(scaler, "scaler.pkl")

# Split data
X = df.drop(columns=["Final Price"])
y = df["Final Price"]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train model (Assuming you're using RandomForestRegressor)
from sklearn.ensemble import RandomForestRegressor
model = RandomForestRegressor(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# Save the trained model to 'dynamic_pricing_model.pkl'
joblib.dump(model, "dynamic_pricing_model.pkl")

# Now you can load the model, scaler, and label encoders
model = joblib.load("dynamic_pricing_model.pkl")
scaler = joblib.load("scaler.pkl")
label_encoders = joblib.load("label_encoders.pkl")
print(model, scaler, label_encoders)

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import mean_absolute_error, r2_score
import joblib

# Load dataset
df = pd.read_csv("dynamic_pricing_data.csv")  # Update this path with the correct one

# Encode categorical variables
label_encoders = {}
for col in ["Product Name", "Category", "Demand", "Season"]:
    le = LabelEncoder()
    df[col] = le.fit_transform(df[col])
    label_encoders[col] = le

# Scale numerical features
scaler = StandardScaler()
num_cols = ["Base Price", "Competitor Price", "Stock", "Reviews", "Rating", "Discount"]
df[num_cols] = scaler.fit_transform(df[num_cols])

# Save label encoders and scaler
joblib.dump(label_encoders, "label_encoders.pkl")
joblib.dump(scaler, "scaler.pkl")

# Split data
X = df.drop(columns=["Final Price"])
y = df["Final Price"]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train model using Gradient Boosting Regressor
model = GradientBoostingRegressor(n_estimators=200, learning_rate=0.1, max_depth=5, random_state=42)
model.fit(X_train, y_train)

# Save the trained model
joblib.dump(model, "dynamic_pricing_model.pkl")

# Evaluate model
y_pred = model.predict(X_test)
print(f"MAE: {mean_absolute_error(y_test, y_pred)}")
print(f"R¬≤ Score: {r2_score(y_test, y_pred)}")

import joblib
model = joblib.load("dynamic_pricing_model.pkl")
scaler = joblib.load("scaler.pkl")
label_encoders = joblib.load("label_encoders.pkl")
print(model, scaler, label_encoders)

from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, r2_score

# Train model
model = RandomForestRegressor(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# Save model
joblib.dump(model, "dynamic_pricing_model.pkl")

# Evaluate model
y_pred = model.predict(X_test)
print(f"MAE: {mean_absolute_error(y_test, y_pred)}")
print(f"R¬≤ Score: {r2_score(y_test, y_pred)}")

import gradio as gr
import numpy as np

# Load trained model, scaler, and label encoders
model = joblib.load("dynamic_pricing_model.pkl")
scaler = joblib.load("scaler.pkl")
label_encoders = joblib.load("label_encoders.pkl")

def predict_price(product_name, category, base_price, competitor_price, demand, stock, reviews, rating, season, discount):
    # Encode categorical features
    category = label_encoders["Category"].transform([category])[0]
    demand = label_encoders["Demand"].transform([demand])[0]
    season = label_encoders["Season"].transform([season])[0]
    product_name = label_encoders["Product Name"].transform([product_name])[0]

    # Scale numerical features
    features = np.array([base_price, competitor_price, stock, reviews, rating, discount]).reshape(1, -1)
    features = scaler.transform(features)

    # Combine features
    final_features = np.concatenate((features.flatten(), [category, demand, season, product_name])).reshape(1, -1)

    # Predict
    predicted_price = model.predict(final_features)[0]
    return f"Optimal Price: ‚Çπ{round(predicted_price, 2)}"

# Create Gradio UI
iface = gr.Interface(
    fn=predict_price,
    inputs=[
        gr.Dropdown(["iPhone 13", "Nike Shoes", "Samsung TV", "Adidas Jacket", "Dell Laptop", "Sony Headphones", "Apple Watch",
                     "LG Refrigerator", "HP Printer", "Bose Speaker"], label="Product Name"),
        gr.Dropdown(["Electronics", "Fashion", "Home Appliances"], label="Category"),
        gr.Number(label="Base Price"),
        gr.Number(label="Competitor Price"),
        gr.Dropdown(["Low", "Medium", "High"], label="Demand"),
        gr.Number(label="Stock"),
        gr.Number(label="Reviews"),
        gr.Number(label="Rating"),
        gr.Dropdown(["Holiday", "Summer", "Winter", "Off-season"], label="Season"),
        gr.Number(label="Discount (%)"),
    ],
    outputs=gr.Textbox(label="Predicted Price"),
    title="üõí Dynamic Pricing Model",
    description="Enter product details to get an AI-predicted price recommendation.",
)

iface.launch()

import pandas as pd
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.neighbors import NearestNeighbors

# Load the synthetic dataset
df = pd.read_csv('/content/synthetic_product_data.csv')  # Update this path with the correct one

# Example of preprocessing categorical data
categorical_features = ['product_condition', 'category']
numeric_features = ['price']

# Create column transformer to preprocess data
preprocessor = ColumnTransformer(
    transformers=[
        ('cat', OneHotEncoder(), categorical_features),
        ('num', 'passthrough', numeric_features)
    ])

# Apply preprocessing to transform the dataset
product_features = preprocessor.fit_transform(df[['product_condition', 'price', 'category']])

# Fit NearestNeighbors model
knn = NearestNeighbors(n_neighbors=5)
knn.fit(product_features)

# Function to recommend similar products
def recommend_products(product_id):
    product = product_features[product_id].reshape(1, -1)
    _, indices = knn.kneighbors(product)
    return df.iloc[indices[0]]

# Example: Recommend products for product with id 3
recommended_products = recommend_products(3)
print(recommended_products)

import pandas as pd
import gradio as gr
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.neighbors import NearestNeighbors
import random

# Load the synthetic dataset
df = pd.read_csv('/content/synthetic_product_data.csv')  # Update this path with the correct one

# Example of preprocessing categorical data
categorical_features = ['product_condition', 'category']
numeric_features = ['price']

# Create column transformer to preprocess data
preprocessor = ColumnTransformer(
    transformers=[
        ('cat', OneHotEncoder(), categorical_features),
        ('num', 'passthrough', numeric_features)
    ])

# Apply preprocessing to transform the dataset
product_features = preprocessor.fit_transform(df[['product_condition', 'price', 'category']])

# Fit NearestNeighbors model
knn = NearestNeighbors(n_neighbors=5)
knn.fit(product_features)

# Function to recommend similar products from the selected category
def recommend_products(category):
    # Filter products from the selected category
    filtered_df = df[df['category'] == category]

    if filtered_df.empty:
        return "No products found in this category."

    # Randomly select a product from the filtered category
    random_product = random.choice(filtered_df.index)

    # Get the feature vector of the selected product
    product = product_features[random_product].reshape(1, -1)

    # Find similar products
    _, indices = knn.kneighbors(product)

    # Get the recommended products (only from the selected category)
    recommended = df.iloc[indices[0]]
    recommended = recommended[recommended['category'] == category]  # Ensure all recommendations are from the selected category

    return recommended[['product_id', 'product_condition', 'price', 'category']]

# Create Gradio interface
def gradio_interface(category):
    recommended_products = recommend_products(category)
    return recommended_products

# Set up Gradio inputs and outputs
category_input = gr.Dropdown(choices=df['category'].unique().tolist(), label="Select Product Category")
output = gr.Dataframe()

# Launch Gradio interface
gr.Interface(fn=gradio_interface,
             inputs=category_input,
             outputs=output,
             live=True,
             title="Product Recommendation System",
             description="Select a product category to get  similar products from the same category").launch()

import gradio as gr
import pandas as pd
import plotly.express as px
import numpy as np
import time

# Load dataset (Assuming a CSV file with relevant data)
data_file = "marketplace_data.csv"

def load_data():
    return pd.read_csv(data_file)

def update_live_data():
    """Simulate real-time updates by adding new interactions."""
    df = load_data()
    new_entry = {
        "Category": np.random.choice(["Electronics", "Plastic", "Metal", "Wood", "Composite"]),
        "LifecycleYears": round(np.random.uniform(1, 20), 2),
        "Price": round(np.random.uniform(10, 500), 2),
        "NumReviews": np.random.randint(0, 1000)
    }
    df = df.append(new_entry, ignore_index=True)
    df.to_csv(data_file, index=False)

def generate_dashboard():
    """Generates interactive circular economy analytics dashboard with live updates."""
    df = load_data()

    # Product Lifecycle Analytics
    lifecycle_fig = px.bar(df.groupby('Category')['LifecycleYears'].mean().reset_index(),
                            x='Category', y='LifecycleYears', title='Average Product Lifecycle by Category')

    # Dynamic Pricing Trends
    price_trend_fig = px.line(df.groupby('Category')['Price'].mean().reset_index(),
                               x='Category', y='Price', title='Average Price Trends by Category')

    # User Engagement Trends
    engagement_fig = px.bar(df.groupby('Category')['NumReviews'].sum().reset_index(),
                             x='Category', y='NumReviews', title='Total User Reviews per Category')

    # Sustainability & Recycling Insights
    df['Sustainability Score'] = np.random.uniform(0, 100, len(df))
    sustainability_fig = px.scatter(df, x='Price', y='Sustainability Score', color='Category',
                                    title='Sustainability Score vs. Product Price')

    return lifecycle_fig, price_trend_fig, engagement_fig, sustainability_fig

iface = gr.Interface(
    fn=generate_dashboard,
    inputs=[],
    outputs=[gr.Plot(label="Product Lifecycle Analytics"),
             gr.Plot(label="Dynamic Pricing Insights"),
             gr.Plot(label="User Engagement Trends"),
             gr.Plot(label="Sustainability & Recycling Insights")],
    live=True,  # Enables real-time updates
    title="‚ôªÔ∏è Circular Economy Analytics Dashboard",
    description="Interactive insights on product lifecycle, pricing, engagement, and sustainability trends."
)

# Simulate real-time data updates
def live_update():
    while True:
        update_live_data()
        time.sleep(5)  # Update every 5 seconds

import threading
threading.Thread(target=live_update, daemon=True).start()

iface.launch()